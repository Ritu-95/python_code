{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ritu-95/python_code/blob/main/Text_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UeyKeQWZjck3"
      },
      "outputs": [],
      "source": [
        "#Importing library\n",
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re # regular Expression\n",
        "# pip install nltk\n",
        "import nltk \n",
        "nltk.download('punkt')\n",
        "# web scrapping \n",
        "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Artificial_intelligence') #request\n",
        "article = scrapped_data.read() # read\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml') #convert to bs4 format\n",
        "\n",
        "paragraphs = parsed_article.find_all('p') # finding \n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    article_text += p.text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article_text"
      ],
      "metadata": {
        "id": "EyWFqEoumZcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cleaing the text\n",
        "processed_article = article_text.lower()\n",
        "processed_article = re.sub('[^a-zA-Z]', ' ', processed_article) # remove /n / and other \n",
        "processed_article = re.sub(r'\\s+', ' ', processed_article) # strip whitespace"
      ],
      "metadata": {
        "id": "pF8Qj4P2mm-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_sentences = nltk.sent_tokenize(processed_article) # tokenazation sentance"
      ],
      "metadata": {
        "id": "RKBSNmf9m3LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = [nltk.word_tokenize(sent) for sent in all_sentences] # word token"
      ],
      "metadata": {
        "id": "dbkiHQHInEjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "h2k3J0CSnM83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(all_words)):\n",
        "    all_words[i] = [w for w in all_words[i] if w not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "ZFjZs8OunYdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words"
      ],
      "metadata": {
        "id": "n1RHNEKTnszC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}